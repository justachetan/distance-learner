{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../src/expB/\")\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import datasets\n",
    "\n",
    "import PIL\n",
    "\n",
    "import matplotlib\n",
    "from mpl_toolkits import mplot3d as p3\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, f1_score, accuracy_score, classification_report\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(\"../src/expB/\")\n",
    "from spheres_v2 import RandomSphere\n",
    "from ptcifar.models import *\n",
    "from learn_mfld_distance import train, test\n",
    "from myNNs import *\n",
    "from datagen.synthetic.multiple.intertwinedswissrolls import IntertwinedSwissRolls\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Learner for Adversarial Examples\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Train distance function with multiple spheres.\n",
    "2. Construct a classifier which classifies based on the closest distance to the spheres.\n",
    "3. Compare this to a standard classifier which learns the classification task directly (instead of learning distance functions).<br>\n",
    "    3.1. Visualization at lower n's and k's for both classifiers<br>\n",
    "    3.2. None of these class for distance regressor. Evaluate at high sigma's<br>\n",
    "    3.3. Standard classifier with none of these class and compare with 3.2<br> -- naive and smarter sampling\n",
    "4. concentric spheres -- std. clf. recreate the paper first, then move to our setting.\n",
    "5. Adversarial attacks can be detected as points which are at least a threshold distance from the closest manifold.\n",
    "6. Compare adversarial robustness via above method and current adversarial robustness methods on classifiers (e.g. Madry et.alâ€™s adversarial training).\n",
    "7. Run above on different synthetic manifolds.Run above on real datasets: MNIST, Cifar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train distance function with multiple spheres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Generate the dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run spheres_v2.py --twospheres --config data_configs/2sphere_expC_k2n3_config.json --save_dir /azuredrive/datasets/expC/test/\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run this cell for if you want to train std. clf. with off mfld label\n",
    "# and smart off mfld. data augmentation\n",
    "## NOTE: Under construction!! (14/06/21)\n",
    "\n",
    "\n",
    "\n",
    "# train_set.class_labels[train_set.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Training a Distance Regressor on the generated 2spheres dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i learn_mfld_distance.py --train --specs ./expt_specs/specs_distlearn_2swrolls_train_trial.json\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_wise_logits_fn = \"/azuredrive/dumps/expC_dist_learner_for_adv_ex/k2n2/actual_dist_learner_w_masked_mse/test_MLPwithNormalization_model{hidden_sizes=512x4,sigmoid_last,batch_norm}_data{2spheres_in_unit_cube;n=2;k=2;actual_distance}/epoch_wise_val_logits.pt\"\n",
    "epoch_wise_logits = torch.load(epoch_wise_logits_fn)\n",
    "\n",
    "epoch_wise_targets_fn = \"/azuredrive/dumps/expC_dist_learner_for_adv_ex/k2n2/actual_dist_learner_w_masked_mse/test_MLPwithNormalization_model{hidden_sizes=512x4,sigmoid_last,batch_norm}_data{2spheres_in_unit_cube;n=2;k=2;actual_distance}/epoch_wise_val_targets.pt\"\n",
    "epoch_wise_targets = torch.load(epoch_wise_targets_fn)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_wise_logits[:, 0, 1], label=\"pred\")\n",
    "plt.plot(epoch_wise_targets[:, 0, 1], label=\"gt\")\n",
    "plt.xlabel(\"num_epochs\")\n",
    "plt.ylabel(\"distance\")\n",
    "plt.title(\"distances (pred and gt) for an off-mfld. pt. on S2 from S1\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(val_set.normed_all_actual_distances.numpy()[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construct a classifier which classifies based on the closest distance to the spheres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Compute and store logits from Distance Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i learn_mfld_distance.py --test --specs ./expt_specs/specs_distlearn_2spheres_expC_test_22062021.json\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(train_set.normed_all_actual_distances.numpy()[:, 0]))\n",
    "print(np.max(train_set.normed_all_distances.numpy()[:, 0]))\n",
    "\n",
    "print(np.max(train_set.all_actual_distances.numpy()[:, 0]))\n",
    "print(np.max(train_set.all_distances.numpy()[:, 0]))\n",
    "\n",
    "print(np.max(train_set.all_actual_distances.numpy()[:, 0]) / train_set.norm_factor)\n",
    "print(np.max(train_set.all_distances.numpy()[:, 0]) / train_set.norm_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Compute predicted labels based on closest sphere to all the points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmin_dist_clf(inputs, targets, class_labels, off_mfld_lbl=2):\n",
    "    \"\"\"\n",
    "        argmin distance based clf for on-manifold points\n",
    "\n",
    "        (currently coded only for at most two spheres)\n",
    "\n",
    "        \n",
    "        :param inputs: logits obtained from a distance regressor\n",
    "        :type inputs: torch.Tensor \n",
    "        :param targets: actual values of distances\n",
    "        :type targets: torch.Tensor\n",
    "        :param class_labels: class labels from the dataset\n",
    "        :type class_labels: torch.Tensor\n",
    "        :param off_mfld_lbl: label for off manifold samples\n",
    "        :type off_mfld_lbl: int\n",
    "        :return: minimum and argmin of predicted and target distances\n",
    "        :rtype: (torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor)\n",
    "    \"\"\"\n",
    "    \n",
    "    OFF_MFLD_LBL = off_mfld_lbl\n",
    "    # selecting on-manifold points for further computations\n",
    "    on_mfld_targets = targets[class_labels != OFF_MFLD_LBL]\n",
    "    on_mfld_inputs = inputs[class_labels != OFF_MFLD_LBL]\n",
    "    \n",
    "    min_pred_dist, pred_argmin = torch.min(on_mfld_inputs, axis=1)\n",
    "    min_true_dist, true_argmin = torch.min(on_mfld_targets, axis=1)\n",
    "\n",
    "    print(classification_report(true_argmin, pred_argmin, target_names=[\"S1\", \"S2\"]))\n",
    "\n",
    "    return min_pred_dist, pred_argmin, min_true_dist, true_argmin\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_fn = \"/azuredrive/dumps/expC_dist_learner_for_adv_ex/k2n3/actual_dist_learner_w_masked_mse/expC_MTMLPwithNormalization_data{2spheres_in_unit_cube;n=2;k=3;actual_distance}/22062021-105958/logits/train/logits.pt\"\n",
    "targets_fn = \"/azuredrive/dumps/expC_dist_learner_for_adv_ex/k2n3/actual_dist_learner_w_masked_mse/expC_MTMLPwithNormalization_data{2spheres_in_unit_cube;n=2;k=3;actual_distance}/22062021-105958/logits/train/targets.pt\"\n",
    "\n",
    "# dataset_fn = \"/azuredrive/datasets/expC/2spheres_k2n3/train_set.pt\"\n",
    "# dataset_fn = \"/data/adv_geom/src/expC/test/data/train\"\n",
    "dataset_fn = \"/azuredrive/dumps/expC_dist_learner_for_adv_ex/swroll_diagnose_k2n3/actual_dist_learner_w_masked_mse/data/test/\"\n",
    "\n",
    "# logits = torch.load(logits_fn)\n",
    "# targets = torch.load(targets_fn)\n",
    "# dataset = torch.load(dataset_fn)\n",
    "dataset = IntertwinedSwissRolls()\n",
    "dataset.load_data(dataset_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = dataset.class_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, pred_classes, _, true_classes = argmin_dist_clf(logits, targets, class_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualizing the boundary of the `argmin`-based distance clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the decision boundary\n",
    "\n",
    "# indices to plot the decision boundary for\n",
    "x_idx, y_idx = 0, 1\n",
    "\n",
    "\n",
    "INPUT_SIZE = 3\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# load the model\n",
    "model_fn = \"/azuredrive/dumps/expC_dist_learner_for_adv_ex/swroll_diagnose_k2n3/actual_dist_learner_w_masked_mse/models/ckpt.pth\"\n",
    "# model_fn = \"/data/adv_geom/src/expC/test/stdclf_vs_distlearn/models/ckpt.pth\"\n",
    "# model_fn = \"/azuredrive/dumps/expC_dist_learner_for_adv_ex/k2n3/actual_dist_learner_w_masked_mse/expC_MTMLPwithNormalization_data{2spheres_in_unit_cube;n=2;k=3;actual_distance}/22062021-105958/models/ckpt.pth\"\n",
    "# model_fn = \"/azuredrive/dumps/expC_dist_learner_for_adv_ex/k2n2/dist_learner/test_MLPwithNormalization_model{hidden_sizes=512x4,sigmoid_last,batch_norm}_data{2spheres_in_unit_cube;n=100;k=2}/07062021-050120/models/test_MLPwithNormalization_model{hidden_sizes=512x4,sigmoid_last,batch_norm}_data{2spheres_in_unit_cube;n=100;k=2}_07062021-050120_val_loss_2.531593505494811e-07_epoch_498.pth\"\n",
    "# model = MLPwithNormalisation(input_size=INPUT_SIZE, output_size=NUM_CLASSES, hidden_sizes=[512] * 4, weight_norm=False, use_tanh=False, use_relu=False)\n",
    "model = MTMLPwithNormalisation(input_size=INPUT_SIZE, output_size=NUM_CLASSES, hidden_sizes=[512] * 4, weight_norm=False, use_tanh=False, use_relu=False)\n",
    "model.load_state_dict(torch.load(model_fn)[\"model_state_dict\"])\n",
    "\n",
    "\n",
    "# generate a dataset over the sample space\n",
    "BATCH_SIZE = 512\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "# dataset = train_set\n",
    "\n",
    "generated_data = np.random.uniform(low=torch.min(dataset.normed_all_points) - 0.05, high=torch.max(dataset.normed_all_points) + 0.05, size=(100000, INPUT_SIZE))\n",
    "generated_data = torch.from_numpy(generated_data).float()\n",
    "\n",
    "\n",
    "# generated_data = torch.from_numpy(np.random.uniform(low=0, high=1, size=(100000, 2))).float()\n",
    "\n",
    "# generated_data = torch.from_numpy(np.random.normal(loc=0.5, scale=1e-3, size=(100000, 100))).float()\n",
    "\n",
    "\n",
    "\n",
    "dummy_labels = torch.from_numpy(np.zeros((100000, NUM_CLASSES))).float()\n",
    "generated_dataset = TensorDataset(generated_data, dummy_labels)\n",
    "dataloader = DataLoader(dataset=generated_dataset, shuffle=False, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "# misc. but needed\n",
    "CUDA = 0\n",
    "device = torch.device(\"cuda:{}\".format(CUDA) if torch.cuda.is_available() and CUDA is not None else \"cpu\")\n",
    "\n",
    "# computing the logits on the generated dataset\n",
    "_, _, _, generated_logits = test(model, dataloader, device, feature_name=\"normed_points\",\\\n",
    "                                 target_name=\"normed_actual_distances\") \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_pred_classes = torch.min(generated_logits, axis=1)[1]\n",
    "# to mark off-manifold points\n",
    "OFF_MFLD_LABEL = 2\n",
    "THRESH = dataset.D / dataset.norm_factor\n",
    "# THRESH = 0.02\n",
    "gen_pred_classes[torch.min(generated_logits, axis=1)[0] >= THRESH] = OFF_MFLD_LABEL\n",
    "\n",
    "gt_dists = None\n",
    "\n",
    "\n",
    "df1 = pd.DataFrame({\"x\": generated_data[:, 0],\\\n",
    "                   \"y\": generated_data[:, 1],\\\n",
    "                   \"z\": generated_data[:, 2],\\\n",
    "                   \"size\": np.array([0.001] * generated_data.shape[0]),\\\n",
    "                   \"color\": gen_pred_classes})\n",
    "col = [\"blue\", \"green\", \"yellow\"]\n",
    "df1[\"color\"] = df1[\"color\"].apply(lambda x: col[int(x)])\n",
    "\n",
    "\n",
    "data_pred_classes = torch.min(logits, axis=1)[1]\n",
    "data_pred_classes[torch.min(logits, axis=1)[0] >= THRESH] = OFF_MFLD_LABEL\n",
    "\n",
    "df2 = pd.DataFrame({\"x\": dataset.normed_all_points[:, 0],\\\n",
    "                   \"y\": dataset.normed_all_points[:, 1],\\\n",
    "                   \"z\": dataset.normed_all_points[:, 2],\\\n",
    "                   \"size\": np.array([0.001] * dataset.normed_all_points.shape[0]),\\\n",
    "                   \"color\": data_pred_classes})\n",
    "col = [\"blue\", \"green\", \"yellow\"]\n",
    "df2[\"color\"] = df2[\"color\"].apply(lambda x: col[int(x)])\n",
    "\n",
    "\n",
    "df = df1.append(df2, ignore_index=True)\n",
    "\n",
    "\n",
    "fig = px.scatter_3d(df, x='x', y='y', z='z',\n",
    "              color='color', size=\"size\", opacity=0.5)\n",
    "fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n",
    "fig.update_traces(marker=dict(size=2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.show()\n",
    "        \n",
    "# col = [\"blue\", \"yellow\"]\n",
    "# col = [\"blue\", \"green\", \"yellow\"]\n",
    "\n",
    "# for i in range(len(col)):\n",
    "#     plt.scatter(generated_data[gen_pred_classes.numpy() == i, x_idx].numpy(), generated_data[gen_pred_classes.numpy() == i, y_idx].numpy(), s=0.01, c=col[i], label=i)\n",
    "#     if (i < 2):\n",
    "#         plt.scatter(dataset.normed_all_points[dataset.class_labels == i, x_idx].numpy(), dataset.normed_all_points[dataset.class_labels == i, y_idx].numpy(), c=col[i], s=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cn = dataset.shifted_S2.normed_x_cn\n",
    "v_1 = dataset.normed_all_points[dataset.S1.N + dataset.shifted_S2.num_neg + 1]\n",
    "v_2 = dataset.normed_all_points[dataset.S1.N + dataset.shifted_S2.N - 3]\n",
    "\n",
    "\n",
    "# b_1 = (1.5 * dataset.S1.r / dataset.norm_factor) * (v_1 - x_cn) / np.linalg.norm(v_1 - x_cn, ord=2)\n",
    "# b_2 = (1.5 * dataset.S1.r / dataset.norm_factor) * (v_2 - x_cn) / np.linalg.norm(v_2 - x_cn, ord=2)\n",
    "b_1 = v_1 - x_cn\n",
    "b_2 = v_2 - x_cn\n",
    "b_2 = b_2 - np.dot(b_1, b_2) * b_1\n",
    "\n",
    "coefficients = torch.from_numpy(np.random.uniform(low=-2, high=2, size=(100000, 2))).float()\n",
    "span = coefficients[:, 0].reshape(-1, 1) * b_1 + coefficients[:, 1].reshape(-1, 1) * b_2\n",
    "span = span + x_cn\n",
    "span = span.float()\n",
    "\n",
    "dummy_labels = torch.from_numpy(np.zeros((100000, NUM_CLASSES))).float()\n",
    "generated_dataset = TensorDataset(span, dummy_labels)\n",
    "dataloader = DataLoader(dataset=generated_dataset, shuffle=False, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "CUDA = 0\n",
    "device = torch.device(\"cuda:{}\".format(CUDA) if torch.cuda.is_available() and CUDA else \"cpu\")\n",
    "\n",
    "# computing the logits on the generated dataset\n",
    "_, _, _, generated_logits = test(model, dataloader, device, feature_name=\"normed_points\",\\\n",
    "                                 target_name=\"normed_actual_distances\") \n",
    "\n",
    "gen_pred_classes = torch.min(generated_logits, axis=1)[1]\n",
    "# to mark off-manifold points\n",
    "OFF_MFLD_LABEL = 2\n",
    "THRESH = dataset.shifted_S2.D / dataset.norm_factor\n",
    "# THRESH = 0.02\n",
    "gen_pred_classes[torch.min(generated_logits, axis=1)[0] >= THRESH] = OFF_MFLD_LABEL\n",
    "\n",
    "gt_dists = None\n",
    "\n",
    "\n",
    "df1 = pd.DataFrame({\"x\": span[:, 0],\\\n",
    "                   \"y\": span[:, 1],\\\n",
    "                   \"z\": span[:, 2],\\\n",
    "                   \"size\": np.array([0.001] * generated_data.shape[0]),\\\n",
    "                   \"color\": gen_pred_classes})\n",
    "col = [\"blue\", \"green\", \"yellow\"]\n",
    "df1[\"color\"] = df1[\"color\"].apply(lambda x: col[int(x)])\n",
    "\n",
    "start = dataset.S1.N + dataset.shifted_S2.num_neg\n",
    "end = dataset.S1.N + dataset.shifted_S2.N\n",
    "\n",
    "df2 = pd.DataFrame({\"x\": dataset.normed_all_points[start:end, 0],\\\n",
    "                   \"y\": dataset.normed_all_points[start:end, 1],\\\n",
    "                   \"z\": dataset.normed_all_points[start:end, 2],\\\n",
    "                   \"size\": np.array([0.001] * (end - start)),\\\n",
    "                   \"color\": [\"black\"]*(end - start)})\n",
    "\n",
    "\n",
    "df = df1.append(df2, ignore_index=True)\n",
    "\n",
    "fig = px.scatter_3d(df, x='x', y='y', z='z',\n",
    "              color='color', size=\"size\", opacity=0.5)\n",
    "fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n",
    "fig.update_traces(marker=dict(size=2))\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = p3.Axes3D(fig)\n",
    "# # ax.view_init(0, 90)\n",
    "\n",
    "# ax.scatter(span[:, 0], span[:, 1], span[:, 2], c=gen_pred_classes, s=.01)\n",
    "# ax.scatter(x_cn[0], x_cn[1], x_cn[2], edgecolors=\"k\", s=30)\n",
    "# ax.scatter(v_1[0], v_1[1], v_1[2], edgecolors=\"k\", s=30)\n",
    "# ax.scatter(v_2[0], v_2[1], v_2[2], edgecolors=\"k\", s=30)\n",
    "# ax.scatter(dataset.normed_all_points[dataset.S1.num_neg:dataset.S1.N, 0],\\\n",
    "#           dataset.normed_all_points[dataset.S1.num_neg:dataset.S1.N, 1],\\\n",
    "#           dataset.normed_all_points[dataset.S1.num_neg:dataset.S1.N, 2], s=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful analysis when `n = 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# below analysis only works currently when data is embedded in 2D\n",
    "assert model.input_size == 2\n",
    "\n",
    "\n",
    "gen_pred_classes = torch.min(generated_logits, axis=1)[1]\n",
    "# to mark off-manifold points\n",
    "OFF_MFLD_LABEL = 2\n",
    "# THRESH = 0.0002\n",
    "THRESH = dataset.S1.genattrs.D / dataset.norm_factor\n",
    "# THRESH = 0.02\n",
    "# THRESH = dataset.max_norm / dataset.norm_factor\n",
    "gen_pred_classes[torch.min(generated_logits, axis=1)[0] >= THRESH] = OFF_MFLD_LABEL\n",
    "\n",
    "gt_dists = None\n",
    "\n",
    "\n",
    "        \n",
    "col = [\"blue\", \"yellow\"]\n",
    "col = [\"blue\", \"green\", \"yellow\"]\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "for i in range(len(col)):\n",
    "    plt.scatter(generated_data[gen_pred_classes.numpy() == i, x_idx].numpy(), generated_data[gen_pred_classes.numpy() == i, y_idx].numpy(), s=0.01, c=col[i], label=i)\n",
    "    if (i < 2):\n",
    "        plt.scatter(dataset.normed_all_points[dataset.class_labels == i, x_idx].numpy(), dataset.normed_all_points[dataset.class_labels == i, y_idx].numpy(), c=col[i], s=0.1)\n",
    "\n",
    "x_min, x_max = -0.2, 0\n",
    "y_min, y_max = 0.8, 1\n",
    "    \n",
    "observe = generated_data[(generated_data[:, 1] > y_min) & (generated_data[:, 1] < y_max) &\\\n",
    "              (generated_data[:, 0] > x_min) & (generated_data[:, 0] < x_max)]\n",
    "observed_logits = generated_logits[(generated_data[:, 1] > y_min) & (generated_data[:, 1] < y_max) &\\\n",
    "              (generated_data[:, 0] > x_min) & (generated_data[:, 0] < x_max)]\n",
    "# plt.scatter(observe[:, 0], observe[:, 1])\n",
    "\n",
    "# plt.scatter(dataset.S1.normed_x_cn[0], dataset.S1.normed_x_cn[1], c=\"black\")\n",
    "# plt.scatter(dataset.shifted_S2.normed_x_cn[0], dataset.shifted_S2.normed_x_cn[1], c=\"black\")\n",
    "\n",
    "\n",
    "plt.legend(markerscale=100)\n",
    "plt.title(\"fig.1: clf labels with off-manifold label (2) for dist regressor\")\n",
    "plt.show()\n",
    "\n",
    "mtype=\"swissroll\"\n",
    "if mtype==\"sphere\":\n",
    "    sphere_objects = [dataset.S1, dataset.shifted_S2]\n",
    "\n",
    "    for i in range(len(sphere_objects)):\n",
    "        if gt_dists is None:\n",
    "            gt_dists = np.abs(np.linalg.norm(generated_data - sphere_objects[i].normed_x_cn, ord=2, axis=1) - (sphere_objects[i].r/dataset.norm_factor))\n",
    "        else:\n",
    "            tmp = np.abs(np.linalg.norm(generated_data - sphere_objects[i].normed_x_cn, ord=2, axis=1) - (sphere_objects[i].r/dataset.norm_factor))\n",
    "            gt_dists = np.vstack((gt_dists, tmp))\n",
    "\n",
    "    gt_dists = gt_dists.T\n",
    "    print(gt_dists.shape)\n",
    "    gt_labels = torch.min(torch.from_numpy(gt_dists), axis=1)[1]\n",
    "\n",
    "    for i in range(len(col)):\n",
    "        plt.scatter(generated_data[gt_labels.numpy() == i, x_idx].numpy(), generated_data[gt_labels.numpy() == i, y_idx].numpy(), s=0.01, c=col[i], label=i)\n",
    "        if (i < 2):\n",
    "            plt.scatter(dataset.normed_all_points[dataset.class_labels == i, x_idx].numpy(), dataset.normed_all_points[dataset.class_labels == i, y_idx].numpy(), c=col[i], s=0.1)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    for i in range(len(sphere_objects)):\n",
    "\n",
    "        fig, axs = plt.subplots(3, 2, figsize=(14, 16))\n",
    "\n",
    "        axs[0, 0].scatter(generated_data[generated_logits[:, i] >= sphere_objects[i].D / dataset.norm_factor][:, 0],\\\n",
    "                   generated_data[generated_logits[:, i] >= sphere_objects[i].D / dataset.norm_factor][:, 1], s=0.01)\n",
    "        axs[0, 0].set_title(\"predicted off mfld pts.\")\n",
    "\n",
    "        axs[0, 1].scatter(generated_data[np.abs(np.linalg.norm(generated_data - sphere_objects[i].normed_x_cn, ord=2, axis=1) - sphere_objects[i].r / dataset.norm_factor) >= sphere_objects[i].D / dataset.norm_factor][:, 0],\\\n",
    "                   generated_data[np.abs(np.linalg.norm(generated_data - sphere_objects[i].normed_x_cn, ord=2, axis=1) - sphere_objects[i].r / dataset.norm_factor) >= sphere_objects[i].D / dataset.norm_factor][:, 1], s=0.01)\n",
    "        axs[0, 1].set_title(\"gt off mfld pts.\".format(a=i+1, b=i))\n",
    "\n",
    "        sc1 = axs[1, 0].scatter(generated_data[:, 0], generated_data[:, 1], s=0.01, c=generated_logits[:, i], cmap=\"hot\")\n",
    "        fig.colorbar(sc1, ax=axs[1, 0])\n",
    "        axs[1, 0].set_title(\"predicted heat map\")\n",
    "\n",
    "\n",
    "    #     gt_dist = np.clip(np.abs(np.linalg.norm(generated_data - sphere_objects[i].normed_x_cn, ord=2, axis=1) - (sphere_objects[i].r/dataset.norm_factor)), 0, sphere_objects[i].D / dataset.norm_factor)\n",
    "        gt_dist = np.abs(np.linalg.norm(generated_data - sphere_objects[i].normed_x_cn, ord=2, axis=1) - (sphere_objects[i].r/dataset.norm_factor))\n",
    "\n",
    "        sc2 = axs[1, 1].scatter(generated_data[:, 0], generated_data[:, 1], s=0.01, c=gt_dist, cmap=\"hot\")\n",
    "        fig.colorbar(sc2, ax=axs[1, 1])\n",
    "        axs[1, 1].set_title(\"gt heat map\")\n",
    "\n",
    "\n",
    "        sc3 = axs[2, 0].scatter(generated_data[:, 0], generated_data[:, 1], s=0.01, c=(generated_logits[:, i] - gt_dist), cmap=\"hot\")\n",
    "        fig.colorbar(sc3, ax=axs[2, 0])\n",
    "        axs[2, 0].set_title(\"signed error heat map\")\n",
    "\n",
    "\n",
    "        sc4 = axs[2, 1].scatter(generated_data[:, 0], generated_data[:, 1], s=0.01, c=(generated_logits[:, i] - gt_dist)**2, cmap=\"hot\")\n",
    "        fig.colorbar(sc4, ax=axs[2, 1])\n",
    "        axs[2, 1].set_title(\"L2 error heat map\")\n",
    "\n",
    "\n",
    "        fig.suptitle(\"fig. 2.{a}: for S{a} (label {b})\".format(a=i+1, b=i))\n",
    "        fig.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "if mtype==\"swissroll\":\n",
    "    swroll_objects = [dataset.S1, dataset.S2]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    for i in range(len(swroll_objects)):\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "        axs[0].scatter(generated_data[generated_logits[:, i] >= swroll_objects[i].genattrs.D / dataset.norm_factor][:, 0],\\\n",
    "                   generated_data[generated_logits[:, i] >= swroll_objects[i].genattrs.D / dataset.norm_factor][:, 1], s=0.01)\n",
    "        axs[0].set_title(\"predicted off mfld pts.\")\n",
    "\n",
    "\n",
    "        sc1 = axs[1].scatter(generated_data[:, 0], generated_data[:, 1], s=0.01, c=generated_logits[:, i], cmap=\"hot\")\n",
    "        fig.colorbar(sc1, ax=axs[1])\n",
    "        axs[1].set_title(\"predicted heat map\")\n",
    "\n",
    "\n",
    "    #     gt_dist = np.clip(np.abs(np.linalg.norm(generated_data - sphere_objects[i].normed_x_cn, ord=2, axis=1) - (sphere_objects[i].r/dataset.norm_factor)), 0, sphere_objects[i].D / dataset.norm_factor)\n",
    "\n",
    "\n",
    "        fig.suptitle(\"fig. 2.{a}: for S{a} (label {b})\".format(a=i+1, b=i))\n",
    "        fig.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(torch.min(dataset.all_distances, axis=1)[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.all_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(torch.min(observed_logits, axis=1)[0].numpy())\n",
    "plt.show()\n",
    "plt.hist(torch.min(observed_logits, axis=1)[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, figsize=(10, 8))\n",
    "\n",
    "axs[0].hist(np.linalg.norm(generated_data[(generated_data[:, 1] > y_min) & (generated_data[:, 1] < y_max) &\\\n",
    "              (generated_data[:, 0] > x_min) & (generated_data[:, 0] < x_max)] - dataset.S1.normed_x_cn, ord=2, axis=1) - (dataset.S1.r/dataset.norm_factor))\n",
    "axs[0].set_title(\"gt\")\n",
    "\n",
    "axs[1].hist(torch.min(generated_logits[(generated_data[:, 1] > y_min) & (generated_data[:, 1] < y_max) &\\\n",
    "              (generated_data[:, 0] > x_min) & (generated_data[:, 0] < x_max)], axis=1)[0].numpy())\n",
    "axs[1].set_title(\"min\")\n",
    "\n",
    "axs[2].hist(torch.max(generated_logits[(generated_data[:, 1] > y_min) & (generated_data[:, 1] < y_max) &\\\n",
    "              (generated_data[:, 0] > x_min) & (generated_data[:, 0] < x_max)], axis=1)[0].numpy())\n",
    "axs[2].set_title(\"max\")\n",
    "fig.suptitle(\"fig. 3: distribution of predicted and ground truth distances from S1 in shaded region in fig. 1\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare this to a standard classifier which learns the classification task directly (instead of learning distance functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Train a standard classifier on the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i learn_mfld_distance.py --train --specs ./expt_specs/specs_stdclf_2spheres_expC_train_23062021.json\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Compute and store logits for standard classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i learn_mfld_distance.py --test --specs ./expt_specs/specs_stdclf_2spheres_expC_test_07062021.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Visualization of Decision Regions for standard classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the decision boundary\n",
    "\n",
    "# indices to plot the decision boundary for\n",
    "x_idx, y_idx = 0, 1\n",
    "\n",
    "INPUT_SIZE = 3\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# load the model\n",
    "# model_fn = \"/azuredrive/dumps/expC_dist_learner_for_adv_ex/k2n2/std_clf/test_MLPwithNormalization_model{hidden_sizes=512x4,sigmoid_last,batch_norm,clf}_data{2spheres_in_unit_cube;n=2;k=2}/07062021-072632/models/test_MLPwithNormalization_model{hidden_sizes=512x4,sigmoid_last,batch_norm,clf}_data{2spheres_in_unit_cube;n=2;k=2}_07062021-072632_val_loss_0.6298815727233886_epoch_2.pth\"\n",
    "# model = MLPwithNormalisation(input_size=INPUT_SIZE, output_size=NUM_CLASSES, hidden_sizes=[512] * 4, weight_norm=False, use_tanh=False, use_relu=False)\n",
    "# model.load_state_dict(torch.load(model_fn)[\"model_state_dict\"])\n",
    "\n",
    "\n",
    "# generate a dataset over the sample space\n",
    "BATCH_SIZE = 512\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "generated_data = torch.from_numpy(np.random.uniform(low=torch.min(dataset.normed_all_points) - 0.01, high=torch.max(dataset.normed_all_points) + 0.01, size=(100000, INPUT_SIZE))).float()\n",
    "# generated_data = torch.from_numpy(np.random.normal(loc=0.5, scale=1e-3, size=(100000, 100))).float()\n",
    "\n",
    "dummy_labels = torch.from_numpy(np.zeros(100000)).float()\n",
    "generated_dataset = TensorDataset(generated_data, dummy_labels)\n",
    "dataloader = DataLoader(dataset=generated_dataset, shuffle=False, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "# misc. but needed\n",
    "CUDA = 0\n",
    "device = torch.device(\"cuda:{}\".format(CUDA) if torch.cuda.is_available() and CUDA else \"cpu\")\n",
    "\n",
    "TASK = \"clf\"\n",
    "FEAT_NAME = \"normed_points\"\n",
    "TGT_NAME = \"classes\"\n",
    "\n",
    "\n",
    "# computing the logits on the generated dataset\n",
    "_, _, _, generated_logits = test(model, dataloader, device, task=TASK, feature_name=FEAT_NAME, target_name=TGT_NAME) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_pred_classes = torch.max(generated_logits, axis=1)[1]\n",
    "# to mark off-manifold points\n",
    "OFF_MFLD_LABEL = 2\n",
    "THRESH = dataset.S1.D / dataset.norm_factor\n",
    "# THRESH = 0.02\n",
    "# gen_pred_classes[torch.min(generated_logits, axis=1)[0] >= THRESH] = OFF_MFLD_LABEL\n",
    "\n",
    "gt_dists = None\n",
    "\n",
    "\n",
    "df1 = pd.DataFrame({\"x\": generated_data[:, 0],\\\n",
    "                   \"y\": generated_data[:, 1],\\\n",
    "                   \"z\": generated_data[:, 2],\\\n",
    "                   \"size\": np.array([0.001] * generated_data.shape[0]),\\\n",
    "                   \"color\": gen_pred_classes})\n",
    "col = [\"blue\", \"green\", \"yellow\"]\n",
    "df1[\"color\"] = df1[\"color\"].apply(lambda x: col[int(x)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = px.scatter_3d(df1, x='x', y='y', z='z',\n",
    "              color='color', size=\"size\", opacity=0.5)\n",
    "fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n",
    "fig.update_traces(marker=dict(size=2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cn = dataset.S1.normed_x_cn\n",
    "# v_1 = dataset.normed_all_points[dataset.S1.N + dataset.shifted_S2.num_neg + 1]\n",
    "# v_2 = dataset.normed_all_points[dataset.S1.N + dataset.shifted_S2.N - 3]\n",
    "v_1 = dataset.normed_all_points[dataset.S1.num_neg + 1]\n",
    "v_2 = dataset.normed_all_points[dataset.S1.N - 3]\n",
    "\n",
    "# b_1 = (1.5 * dataset.S1.r / dataset.norm_factor) * (v_1 - x_cn) / np.linalg.norm(v_1 - x_cn, ord=2)\n",
    "# b_2 = (1.5 * dataset.S1.r / dataset.norm_factor) * (v_2 - x_cn) / np.linalg.norm(v_2 - x_cn, ord=2)\n",
    "b_1 = v_1 - x_cn\n",
    "b_2 = v_2 - x_cn\n",
    "b_2 = b_2 - np.dot(b_1, b_2) * b_1\n",
    "\n",
    "coefficients = torch.from_numpy(np.random.uniform(low=-2, high=2, size=(100000, 2))).float()\n",
    "span = coefficients[:, 0].reshape(-1, 1) * b_1 + coefficients[:, 1].reshape(-1, 1) * b_2\n",
    "span = span + x_cn\n",
    "span = span.float()\n",
    "\n",
    "dummy_labels = torch.from_numpy(np.zeros((100000, NUM_CLASSES))).float()\n",
    "generated_dataset = TensorDataset(span, dummy_labels)\n",
    "dataloader = DataLoader(dataset=generated_dataset, shuffle=False, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "CUDA = 0\n",
    "device = torch.device(\"cuda:{}\".format(CUDA) if torch.cuda.is_available() and CUDA else \"cpu\")\n",
    "\n",
    "# computing the logits on the generated dataset\n",
    "_, _, _, generated_logits = test(model, dataloader, device, feature_name=\"normed_points\",\\\n",
    "                                 target_name=\"classes\") \n",
    "\n",
    "gen_pred_classes = torch.min(generated_logits, axis=1)[1]\n",
    "# to mark off-manifold points\n",
    "OFF_MFLD_LABEL = 2\n",
    "THRESH = dataset.shifted_S2.D / dataset.norm_factor\n",
    "# THRESH = 0.02\n",
    "# gen_pred_classes[torch.min(generated_logits, axis=1)[0] >= THRESH] = OFF_MFLD_LABEL\n",
    "\n",
    "gt_dists = None\n",
    "\n",
    "\n",
    "df1 = pd.DataFrame({\"x\": span[:, 0],\\\n",
    "                   \"y\": span[:, 1],\\\n",
    "                   \"z\": span[:, 2],\\\n",
    "                   \"size\": np.array([0.001] * generated_data.shape[0]),\\\n",
    "                   \"color\": gen_pred_classes})\n",
    "col = [\"blue\", \"green\", \"yellow\"]\n",
    "df1[\"color\"] = df1[\"color\"].apply(lambda x: col[int(x)])\n",
    "\n",
    "# start = dataset.S1.N + dataset.shifted_S2.num_neg\n",
    "# end = dataset.S1.N + dataset.shifted_S2.N\n",
    "start = dataset.S1.num_neg\n",
    "end = dataset.S1.N\n",
    "\n",
    "\n",
    "df2 = pd.DataFrame({\"x\": dataset.normed_all_points[start:end, 0],\\\n",
    "                   \"y\": dataset.normed_all_points[start:end, 1],\\\n",
    "                   \"z\": dataset.normed_all_points[start:end, 2],\\\n",
    "                   \"size\": np.array([0.001] * (end - start)),\\\n",
    "                   \"color\": [\"black\"]*(end - start)})\n",
    "\n",
    "\n",
    "df = df1.append(df2, ignore_index=True)\n",
    "\n",
    "fig = px.scatter_3d(df, x='x', y='y', z='z',\n",
    "              color='color', size=\"size\", opacity=0.5)\n",
    "fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n",
    "fig.update_traces(marker=dict(size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_pred_classes = torch.max(generated_logits, axis=1)[1]\n",
    "\n",
    "plt.scatter(generated_data[:, x_idx].numpy(), generated_data[:, y_idx].numpy(), c=gen_pred_classes.numpy(), s=0.01, cmap=matplotlib.cm.cool)\n",
    "\n",
    "OFF_MFLD_LBL = 2\n",
    "plt.scatter(dataset.normed_all_points[dataset.class_labels != OFF_MFLD_LBL, x_idx].numpy(), dataset.normed_all_points[dataset.class_labels != OFF_MFLD_LBL, y_idx].numpy(), c=dataset.class_labels[dataset.class_labels != OFF_MFLD_LBL], cmap=matplotlib.cm.cool, s=0.1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Compare performance for Standard classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above results in 3.2 and 2.2 that both the standard classifier and the `argmin`-distance-based classifiers perform equally well for manifold classification.\n",
    "\n",
    "In order to stress-test and compare these two setups, we add **random gaussian noise to the inputs** and see how well the models take that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_fns = {\n",
    "    \"train\": \"/azuredrive/datasets/expC/2spheres/train_set.pt\",\n",
    "    \"val\": \"/azuredrive/datasets/expC/2spheres/val_set.pt\",\n",
    "    \"test\": \"/azuredrive/datasets/expC/2spheres/test_set.pt\"\n",
    "}\n",
    "\n",
    "INPUT_SIZE = 2\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "dr_model_fn = \"/azuredrive/dumps/expC_dist_learner_for_adv_ex/dist_learner/test_MLPwithNormalization_model{hidden_sizes=512x4,sigmoid_last,batch_norm}_data{2spheres_in_unit_cube;n=100;k=2}/31052021-192633/models/test_MLPwithNormalization_model{hidden_sizes=512x4,sigmoid_last,batch_norm}_data{2spheres_in_unit_cube;n=100;k=2}_31052021-192633_val_loss_9.026023826663732e-06_epoch_499.pth\"\n",
    "dr_model = MLPwithNormalisation(input_size=INPUT_SIZE, output_size=NUM_CLASSES, hidden_sizes=[512] * 4, weight_norm=False, use_tanh=False, use_relu=False)\n",
    "dr_model.load_state_dict(torch.load(dr_model_fn)[\"model_state_dict\"])\n",
    "\n",
    "sc_model_fn = \"/azuredrive/dumps/expC_dist_learner_for_adv_ex/std_clf/test_MLPwithNormalization_model{hidden_sizes=512x4,sigmoid_last,batch_norm,clf}_data{2spheres_in_unit_cube;n=100;k=2}/01062021-172847/models/test_MLPwithNormalization_model{hidden_sizes=512x4,sigmoid_last,batch_norm,clf}_data{2spheres_in_unit_cube;n=100;k=2}_01062021-172847_val_loss_0.6836836189031601_epoch_2.pth\"\n",
    "sc_model = MLPwithNormalisation(input_size=INPUT_SIZE, output_size=NUM_CLASSES, hidden_sizes=[512] * 4, weight_norm=False, use_tanh=False, use_relu=False)\n",
    "sc_model.load_state_dict(torch.load(sc_model_fn)[\"model_state_dict\"])\n",
    "\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "CUDA = 0\n",
    "device = torch.device(\"cuda:{}\".format(CUDA) if torch.cuda.is_available() and CUDA else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    \n",
    "    dataset = torch.load(data_fns[split])\n",
    "\n",
    "    # Add Gaussian noise\n",
    "    STDDEV = 1e-1\n",
    "    MU = 0\n",
    "    \n",
    "#     scale = 1e-3\n",
    "    noise = np.random.normal(MU, STDDEV, size=dataset.normed_all_points.shape[1])\n",
    "#     noise = scale * (noise / np.linalg.norm(noise, ord=2))\n",
    "    dataset.normed_all_points = (dataset.normed_all_points + noise).float()\n",
    "    print(\"noise scale:\", np.linalg.norm(noise, ord=2))\n",
    "    \n",
    "\n",
    "    ## for distance regressor\n",
    "    print(\"distance regressor; split:\", split)\n",
    "    # Compute logits\n",
    "    \n",
    "    dataloader = DataLoader(dataset, shuffle=False, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "    _, _, targets, generated_logits = test(dr_model, dataloader, device) \n",
    "    class_labels = dataset.class_labels\n",
    "    \n",
    "    # Compute performance\n",
    "    _, pred_classes, _, true_classes = argmin_dist_clf(generated_logits, targets, class_labels)\n",
    "    \n",
    "    \n",
    "    ## for standard classifier\n",
    "    print(\"standard classifier; split:\", split)\n",
    "    \n",
    "    OFF_MFLD_LABEL = 2\n",
    "\n",
    "    dataset.all_points = dataset.all_points[dataset.class_labels != OFF_MFLD_LABEL]\n",
    "    dataset.all_distances = dataset.all_distances[dataset.class_labels != OFF_MFLD_LABEL]\n",
    "    dataset.normed_all_points = dataset.normed_all_points[dataset.class_labels != OFF_MFLD_LABEL]\n",
    "    dataset.normed_all_distances = dataset.normed_all_distances[dataset.class_labels != OFF_MFLD_LABEL]\n",
    "    dataset.class_labels = dataset.class_labels[dataset.class_labels != OFF_MFLD_LABEL]\n",
    "\n",
    "    \n",
    "    # Compute logits\n",
    "    \n",
    "    dataloader = DataLoader(dataset, shuffle=False, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "    _, _, targets, generated_logits = test(sc_model, dataloader, device, task=\"clf\", feature_name=\"normed_points\", target_name=\"classes\", name=\"std. clf\") \n",
    "    class_labels = dataset.class_labels\n",
    "\n",
    "    # Compute performance\n",
    "    ## already done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
