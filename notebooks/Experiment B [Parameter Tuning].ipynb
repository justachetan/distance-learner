{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn import functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import shutil\n",
    "import tempfile\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.utilities.cloud_io import load as pl_load\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback, \\\n",
    "    TuneReportCheckpointCallback\n",
    "\n",
    "\n",
    "\n",
    "# adding relevant files to PATH\n",
    "sys.path.append(\"../src/expB/\")\n",
    "\n",
    "from ptcifar.models import ResNet18\n",
    "import spheres\n",
    "from myNNs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18MfldDistRegressor(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    This has been adapted from\n",
    "    https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    def __init__(self, config, data_dir=None):\n",
    "        \n",
    "        \n",
    "        super(ResNet18MfldDistRegressor, self).__init__()\n",
    "\n",
    "        self.data_dir = data_dir or os.getcwd()\n",
    "        \n",
    "        self.lr = config[\"lr\"]\n",
    "        self.momentum = config[\"momentum\"]\n",
    "        \n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.optimizer_type = config[\"optimizer_type\"]\n",
    "        self.scheduler_params = config[\"scheduler_params\"]\n",
    "        \n",
    "#         self.train_set = config[\"train_set\"]\n",
    "#         self.val_set = config[\"val_set\"]\n",
    "\n",
    "        \n",
    "        self.train_epoch_losses = list()\n",
    "        \n",
    "        self.num_epochs = config[\"num_epochs\"]\n",
    "        \n",
    "        # defining the model\n",
    "        self.model = ResNet18(num_classes=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.model(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    def mean_squared_error(self, logits, labels):\n",
    "        return F.mse_loss(logits, labels)\n",
    "\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.mean_squared_error(logits, y)\n",
    "\n",
    "#         self.log(\"ptl/train_loss\", loss)\n",
    "        self.train_epoch_losses.append(loss.item())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.mean_squared_error(logits, y)\n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_val_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        avg_train_loss = np.mean(self.train_epoch_losses)\n",
    "        self.log(\"ptl/val_loss\", avg_val_loss)\n",
    "        self.log(\"ptl/train_loss\", avg_train_loss)\n",
    "        self.train_epoch_losses = list()\n",
    "\n",
    "    @staticmethod\n",
    "    def download_data(data_dir):\n",
    "\n",
    "        n = 32 * 32 * 3\n",
    "\n",
    "        train_params = {\n",
    "            \"N\": 50000,\n",
    "            \"num_neg\": None,\n",
    "            \"n\": n,\n",
    "            \"k\": 2,\n",
    "            \"r\": 100.0,\n",
    "            \"D\": 25.0,\n",
    "            \"max_norm\": 500.0,\n",
    "            \"mu\": 1000,\n",
    "            \"sigma\": 5000,\n",
    "            \"seed\": 23\n",
    "        }\n",
    "\n",
    "        train_set = spheres.RandomSphere(**train_params)\n",
    "\n",
    "        val_params = {\n",
    "            \"N\": 10000,\n",
    "            \"num_neg\": None,\n",
    "            \"n\": n,\n",
    "            \"k\": 2,\n",
    "            \"r\": 100.0,\n",
    "            \"D\": 25.0,\n",
    "            \"max_norm\": 500.0,\n",
    "            \"mu\": 1000,\n",
    "            \"sigma\": 5000,\n",
    "            \"seed\": 101,\n",
    "            \"x_ck\": train_set.x_ck,\n",
    "            \"translation\": train_set.translation,\n",
    "            \"rotation\": train_set.rotation\n",
    "        }\n",
    "        \n",
    "        val_set = spheres.RandomSphere(**val_params)\n",
    "        \n",
    "        torch.save(train_set, os.path.join(self.data_dir, \"train_cifar_dim.pt\"))\n",
    "        torch.save(val_set, os.path.join(self.data_dir, \"val_cifar_dim.pt\"))\n",
    "        \n",
    "        return train_set, val_set\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.train_set, self.val_set = self.download_data(self.data_dir)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=int(self.batch_size))\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size=int(self.batch_size))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = None\n",
    "        if self.optimizer_type == \"adam\":\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        elif self.optimizer_type == \"sgd\":\n",
    "            optimizer = torch.optim.SGD(self.parameters(), lr=self.lr, momentum=self.momentum)\n",
    "        \n",
    "        lr_sched_factor = lambda epoch: epoch / (self.scheduler_params[\"warmup\"]) if epoch <= self.scheduler_params[\"warmup\"] else (1 if epoch > self.scheduler_params[\"warmup\"] and epoch < self.scheduler_params[\"cooldown\"] else max(0, 1 + (1 / (self.scheduler_params[\"cooldown\"] - self.num_epochs)) * (epoch - self.scheduler_params[\"cooldown\"])))\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_sched_factor)\n",
    "        \n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "def train_mnist(config, data_dir, num_epochs):\n",
    "    model = ResNet18MfldDistRegressor(config, data_dir)\n",
    "    trainer = pl.Trainer(max_epochs=num_epochs, gpus=1)\n",
    "\n",
    "    trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_tune_checkpoint(config,\n",
    "                                checkpoint_dir=None,\n",
    "                                data_dir=None,\n",
    "                                num_epochs=10,\n",
    "                                num_gpus=1):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=num_epochs,\n",
    "        gpus=num_gpus,\n",
    "        logger=TensorBoardLogger(\n",
    "            save_dir=tune.get_trial_dir(), name=\"\", version=\".\"),\n",
    "        callbacks=[\n",
    "            TuneReportCheckpointCallback(\n",
    "                metrics={\n",
    "                    \"val_loss\": \"ptl/val_loss\"\n",
    "                },\n",
    "                filename=\"checkpoint\",\n",
    "                on=\"validation_end\"),\n",
    "            LearningRateMonitor(logging_interval='epoch', log_momentum=True)\n",
    "        ])\n",
    "    if checkpoint_dir:\n",
    "        # Currently, this leads to errors:\n",
    "        # model = LightningMNISTClassifier.load_from_checkpoint(\n",
    "        #     os.path.join(checkpoint, \"checkpoint\"))\n",
    "        # Workaround:\n",
    "        ckpt = pl_load(\n",
    "            os.path.join(checkpoint_dir, \"checkpoint\"),\n",
    "            map_location=lambda storage, loc: storage)\n",
    "        model = ResNet18MfldDistRegressor._load_model_state(\n",
    "            ckpt, config=config, data_dir=data_dir)\n",
    "        trainer.current_epoch = ckpt[\"epoch\"]\n",
    "    else:\n",
    "        model = ResNet18MfldDistRegressor(config=config, data_dir=data_dir)\n",
    "\n",
    "    trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(data_dir, num_samples=1, num_epochs=5, gpus_per_trial=1):\n",
    "    \n",
    "    \n",
    "#     ResNet18MfldDistRegressor.download_data(data_dir)\n",
    "\n",
    "\n",
    "    \n",
    "    config = {\n",
    "        \"lr\": tune.grid_search([1e-2, 1e-3]),\n",
    "        \"batch_size\": tune.grid_search([512]),\n",
    "        \"optimizer_type\": tune.grid_search([\"sgd\"]),\n",
    "        \"momentum\": tune.grid_search([0.2]),\n",
    "        \"scheduler_params\": {\"warmup\": 10, \"cooldown\": 300},\n",
    "        \"num_epochs\": num_epochs\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "    reporter = CLIReporter(\n",
    "        parameter_columns=[\"lr\", \"momentum\", \"batch_size\", \"optimizer_type\"],\n",
    "        metric_columns=[\"loss\", \"training_iteration\"])\n",
    "\n",
    "    analysis = tune.run(\n",
    "        tune.with_parameters(\n",
    "            train_model_tune_checkpoint,\n",
    "            data_dir=data_dir,\n",
    "            num_epochs=num_epochs,\n",
    "            num_gpus=gpus_per_trial),\n",
    "        resources_per_trial={\n",
    "            \"cpu\": 3,\n",
    "            \"gpu\": gpus_per_trial\n",
    "        },\n",
    "        metric=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        progress_reporter=reporter,\n",
    "        name=\"tune_dist_learn_resnet18\")\n",
    "\n",
    "    print(\"Best hyperparameters found were: \", analysis.best_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "DATA_DIR = \"/data/adv_geom/datasets/expB_tuning/\"\n",
    "\n",
    "# config = {\n",
    "    \n",
    "#     \"lr\": 1e-3,\n",
    "#     \"momentum\": 0.9,\n",
    "#     \"batch_size\": 512,\n",
    "#     \"optimizer_type\": \"sgd\",\n",
    "#     \"scheduler_params\": {\"warmup\": 10, \"cooldown\": 300},\n",
    "#     \"num_epochs\": NUM_EPOCHS\n",
    "    \n",
    "    \n",
    "# }\n",
    "\n",
    "# train_mnist(config, DATA_DIR, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 19.4/220.4 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/12 CPUs, 1/2 GPUs, 0.0/134.77 GiB heap, 0.0/42.58 GiB objects (0/1.0 accelerator_type:V100)\n",
      "Result logdir: /root/ray_results/tune_dist_learn_resnet18\n",
      "Number of trials: 1/2 (1 RUNNING)\n",
      "+-------------------+----------+-------+------+------------+--------------+------------------+\n",
      "| Trial name        | status   | loc   |   lr |   momentum |   batch_size | optimizer_type   |\n",
      "|-------------------+----------+-------+------+------------+--------------+------------------|\n",
      "| inner_f9e5b_00000 | RUNNING  |       | 0.01 |        0.2 |          512 | sgd              |\n",
      "+-------------------+----------+-------+------+------------+--------------+------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-12 09:32:20,299\tWARNING worker.py:1107 -- Failed to unpickle actor class 'ImplicitFunc' for actor ID cc0320e00aa0584cd481d0b901000000. Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/function_manager.py\", line 496, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/cloudpickle/cloudpickle.py\", line 562, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 'spheres'\n",
      "\n",
      "2021-04-12 09:32:20,300\tERROR trial_runner.py:616 -- Trial inner_f9e5b_00001: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/tune/trial_runner.py\", line 586, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/tune/ray_trial_executor.py\", line 609, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/worker.py\", line 1456, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=28332, ip=172.17.0.4)\n",
      "  File \"python/ray/_raylet.pyx\", line 439, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 473, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 476, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 480, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 432, in ray._raylet.execute_task.function_executor\n",
      "RuntimeError: The actor with name ImplicitFunc failed to be imported, and so cannot execute this method.\n",
      "2021-04-12 09:32:20,300\tWARNING worker.py:1107 -- Failed to unpickle actor class 'ImplicitFunc' for actor ID 24eed4584329c19ada30d62101000000. Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/function_manager.py\", line 496, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/cloudpickle/cloudpickle.py\", line 562, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 'spheres'\n",
      "\n",
      "2021-04-12 09:32:20,305\tERROR trial_runner.py:616 -- Trial inner_f9e5b_00000: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/tune/trial_runner.py\", line 586, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/tune/ray_trial_executor.py\", line 609, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/worker.py\", line 1456, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=28339, ip=172.17.0.4)\n",
      "  File \"python/ray/_raylet.pyx\", line 439, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 473, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 476, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 480, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 432, in ray._raylet.execute_task.function_executor\n",
      "RuntimeError: The actor with name ImplicitFunc failed to be imported, and so cannot execute this method.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for inner_f9e5b_00001:\n",
      "  {}\n",
      "  \n",
      "Result for inner_f9e5b_00000:\n",
      "  {}\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 19.3/220.4 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/12 CPUs, 0/2 GPUs, 0.0/134.77 GiB heap, 0.0/42.58 GiB objects (0/1.0 accelerator_type:V100)\n",
      "Result logdir: /root/ray_results/tune_dist_learn_resnet18\n",
      "Number of trials: 2/2 (2 ERROR)\n",
      "+-------------------+----------+-------+-------+------------+--------------+------------------+\n",
      "| Trial name        | status   | loc   |    lr |   momentum |   batch_size | optimizer_type   |\n",
      "|-------------------+----------+-------+-------+------------+--------------+------------------|\n",
      "| inner_f9e5b_00000 | ERROR    |       | 0.01  |        0.2 |          512 | sgd              |\n",
      "| inner_f9e5b_00001 | ERROR    |       | 0.001 |        0.2 |          512 | sgd              |\n",
      "+-------------------+----------+-------+-------+------------+--------------+------------------+\n",
      "Number of errored trials: 2\n",
      "+-------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name        |   # failures | error file                                                                                                                                           |\n",
      "|-------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| inner_f9e5b_00000 |            1 | /root/ray_results/tune_dist_learn_resnet18/inner_f9e5b_00000_0_batch_size=512,lr=0.01,momentum=0.2,optimizer_type=sgd_2021-04-12_09-32-19/error.txt  |\n",
      "| inner_f9e5b_00001 |            1 | /root/ray_results/tune_dist_learn_resnet18/inner_f9e5b_00001_1_batch_size=512,lr=0.001,momentum=0.2,optimizer_type=sgd_2021-04-12_09-32-19/error.txt |\n",
      "+-------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=28332)\u001b[0m 2021-04-12 09:32:20,295\tERROR function_manager.py:498 -- Failed to load actor class ImplicitFunc.\n",
      "\u001b[2m\u001b[36m(pid=28332)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=28332)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/ray/function_manager.py\", line 496, in _load_actor_class_from_gcs\n",
      "\u001b[2m\u001b[36m(pid=28332)\u001b[0m     actor_class = pickle.loads(pickled_class)\n",
      "\u001b[2m\u001b[36m(pid=28332)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/ray/cloudpickle/cloudpickle.py\", line 562, in subimport\n",
      "\u001b[2m\u001b[36m(pid=28332)\u001b[0m     __import__(name)\n",
      "\u001b[2m\u001b[36m(pid=28332)\u001b[0m ModuleNotFoundError: No module named 'spheres'\n",
      "\u001b[2m\u001b[36m(pid=28339)\u001b[0m 2021-04-12 09:32:20,296\tERROR function_manager.py:498 -- Failed to load actor class ImplicitFunc.\n",
      "\u001b[2m\u001b[36m(pid=28339)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=28339)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/ray/function_manager.py\", line 496, in _load_actor_class_from_gcs\n",
      "\u001b[2m\u001b[36m(pid=28339)\u001b[0m     actor_class = pickle.loads(pickled_class)\n",
      "\u001b[2m\u001b[36m(pid=28339)\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/ray/cloudpickle/cloudpickle.py\", line 562, in subimport\n",
      "\u001b[2m\u001b[36m(pid=28339)\u001b[0m     __import__(name)\n",
      "\u001b[2m\u001b[36m(pid=28339)\u001b[0m ModuleNotFoundError: No module named 'spheres'\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [inner_f9e5b_00000, inner_f9e5b_00001])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6227429d067d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtune_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-fbb312446f21>\u001b[0m in \u001b[0;36mtune_model\u001b[0;34m(data_dir, num_samples, num_epochs, gpus_per_trial)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprogress_reporter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreporter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         name=\"tune_dist_learn_resnet18\")\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best hyperparameters found were: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [inner_f9e5b_00000, inner_f9e5b_00001])"
     ]
    }
   ],
   "source": [
    "tune_model(data_dir=DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
